# -*- coding: utf-8 -*-
"""BIOS_534_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y4YOZdMNOl7lMASkez5TzLSCyaQotzie
"""



"""# BIOS 534 - Regression

This note book provides a basic equivalent to the R code presented in class on 3/26. This is not intended to be a one-to-one, lock step reproduction of the R code because, of necessity, the code flow will be different because Python is a different language with its own way of doing things. So it's not possible to make things look the same between the two languages nor should one even try. Each has its strengths. The larger point(s) of the notebook is to show the following:

*   How to Build A Regression Model And Compute RMSE
*   How To Do K Fold Validation Using a Helper Function
*   How To Do K Fold Validation Manually
"""

# Import some needed modules

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

"""# A Basic Example

So we'll start by building a Regression model on the entire mtcars data frame. We did this with the R example. Ordinarily, you wouldn't do this but this gives you an idea of how you would build the model. We'll worry about train / test and cross validation later in this notebook.

## Read In The Data

Next, we read in the mtcars data frame from the Internet
"""

# Read In The mtcars data frame
url = "https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/mtcars.csv"
dfmt = pd.read_csv(url) 
dfmt.info()

"""## Identify The Target Feature

So we need to determine what the target / outcome feature is and what the predictor features are. We want to predict mpg as a function of weight so we need to index into the data frame to get those values.
"""

# Identify the Y / outcome feature
Y = dfmt.mpg

# Identify the predictor features. Since we are using just a single column 
# we have to reshape it. Don't know why but that's the way it is

X = np.ravel(dfmt.wt).reshape(-1,1)

# Print them Out
Y

"""## Create The Linear Regression Object

Now we will create a Linear Regression object and start working with that. We'll first build the model using the entire data set itself. Ordinarily, this isn't something that you would do but this is just a first example.
"""

# Initialize a Linear Regression object
model = LinearRegression()

# Fit the model with the training data
model.fit(X,Y)

# Get the coefficeints of the trained model
print('\nCoefficient of model :', model.coef_)

# Get the intercept
print('slope:', model.intercept_)

# R-squared
r_sq = model.score(X, Y)
print('R-Squared:', r_sq.round(3))

"""## Making Some Predictions

Look at the predictions using the training data. We are using the same dataset that we trained the model on.
"""

predict_train = model.predict(X)
print('\nMPG on training data',predict_train) 

# Root Mean Squared Error on training dataset
rmse_train = mean_squared_error(Y,predict_train)**(0.5)
print('\nRMSE on training dataset : ', rmse_train.round(2))

"""# Use a Training / Test Pair

So as we learned in the lecture, a reasonable next step is to create a Train / Test pair which would help provide a better estiamte of out-of-sample error than training purely on the data set. Just as R has some helper functions for this, so does Python.
"""

# Import the train / test module
from sklearn.model_selection import train_test_split

# We'll use an 80/20 split
seed = 7
test_size = .20

# The train_test_split function does the work
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)

# Check the sizes of the Train Data
X_train.size
Y_train.size

"""## Repeat The Regression

So we'll do the same as before except this time we'll train the model using only the training subset. Then we'll see what the RMSE is on the training data. After that we'll then use it on the test data to see if the RMSE on the test data is similar.
"""

# Initialize another Linear Regression object
model2 = LinearRegression()

# Fit the model with the training data
model2.fit(X_train,Y_train)

# Look at the predictions using the train data
predict_train = model2.predict(X_train)
print('\nMPG on training data',predict_train) 

# Root Mean Squared Error on training dataset
rmse_train = mean_squared_error(Y_train,predict_train)**(0.5)
print('\nRMSE on train dataset : ', rmse_train)

"""## RMSE On The Test Data

So now let's look at the performance on the test data set.
"""

# Predict the target feature using the testing dataset
predict_test = model2.predict(X_test)
print('\nMPG on test data',predict_test) 

# Now compute the Root Mean Squared Error on the test dataset
rmse_test = mean_squared_error(Y_test,predict_test)**(0.5)
print('\nRMSE on test dataset : ', rmse_test)

"""# Assisted Cross Fold Validation

We'll use a convenient function from the sklearn library called cross_validate to help us. This is somewhat similar to the train function from the R caret package in that it automates the creation of a series of train / test data splits.
"""

# Import the helper function
from sklearn.model_selection import cross_validate
from sklearn.model_selection import KFold


# Create a Kfold object - K=4 folds
kfold = KFold(n_splits=4, random_state=7, shuffle=True)

"""## The Kold Function

The KFold function will manage the creation of the folds, in this case 4 of them.  We get back the indices corresponding to rows of the training data. We also get back the indices corresponding to the holdout or test fold
"""

for train_index, test_index in kfold.split(X,Y):
      print(train_index)
      print(test_index,"\n")

"""## The cross_valdiate function

Next up we will initialize the model and use the cross_validate function to manage the creation of 4 models, one for each fold, and then get a sense of the RMSE for each of the four models.
"""

from sklearn.metrics import make_scorer

# Create a Linear Regression Object
model3 = LinearRegression()

# We have to designate a scoring metric 
mse = make_scorer(mean_squared_error)

# The cross_validate function handles the execution of the model
cv  = cross_validate(model3,X_train,Y_train,scoring=(mse), cv=kfold)

# Here we print out the rmse of the test / holdout data
print((cv['test_score']**0.5))

# Look at the mean of this RMSE array
print((cv['test_score']**0.5).mean().round(2))

"""# Manual Cross Fold Validation 

The following is what you might do if you wanted to do the cross fold valdiation on your own without using the convenience of the cross_validate function. This is useful if you want finer grained control over the individual per fold models and perhaps wanted to make predictions on the hold out fold as part of the loop.

## Loop Through The Folds

Here we can create a loop that processes each of the folds. Just to summarize, here is what this process is doing. In reality, this is the same process as in the most recent section just that here we are writing our own loop

1.   We split up the training set into K folds (4 here)
2.   A Model is trained usinf K-1 folds as Training Data
3.   The remaining fold is used as a test data set
4.   Make a prediction on the test data and compute the RMSE
5.   Steps 2 -4 are repeated until all folds have been used as test data 
6.   At the end, return the vector containing all compute RMSEs
"""

# Set up some vectors to collect info

rmse_train_info = np.empty(0)
rmse_test_info = np.empty(0)
ii = 1

# Main processing loop for the folds

for train_index, test_index in kfold.split(X, Y):
  # split data coming from each of the 4 folds
        X_train, X_test = X[train_index], X[test_index]
        Y_train, Y_test = Y[train_index], Y[test_index]
        
        # Initialize a model
        regress = LinearRegression()
        
        # Fit the Model
        regress.fit(X_train,Y_train)
        
        # Predict the target on the training dataset
        predict_train = regress.predict(X_train)
  
        # Root Mean Squared Error on training dataset
        rmse_train = mean_squared_error(Y_train,predict_train)**(0.5)
        print("Training RMSE for loop",ii,"is:", rmse_train)
       
        # Append the rmse to the rmse_train_info vector
        rmse_train_info = np.append(rmse_train_info,rmse_train)
        
        # Now let's do a prediction on the test data
        predict_test = regress.predict(X_test)
        
        # Root Mean Squared Error on training dataset
        rmse_test = mean_squared_error(Y_test,predict_test)**(0.5)
        print("Test RMSE for loop",ii,"is:", rmse_test,"\n")
       
        # Append the rmse to the rmse_test_info vector
        rmse_test_info = np.append(rmse_test_info,rmse_test)

        ii = ii+1

"""## Compare the RMSE Outcomes

As part of the loop we captured the RMSE values for the the training and test folds. We can look at the vectors and compute some basic statistics on them.
"""

print("RMSE For Training Folds", rmse_train_info)

print("RMSE For Test Folds",rmse_test_info)

"""Next what is the mean of these vectors:"""

print("RMSE Training mean:",rmse_train_info.mean())
print("RMSE Test mean:",rmse_test_info.mean())